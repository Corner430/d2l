{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **让训练更加稳定**\n",
    "- 目标：让梯度值在合理的范围内\n",
    "  - 例如[1e-6, 1e-3]\n",
    "- **将乘法变加法**\n",
    "  - ResNet, LSTM\n",
    "- **归一化**\n",
    "  - 梯度归一化，梯度裁剪\n",
    "- **合理的权重初始和激活函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **让每层的方差是一个常数**\n",
    "- 将每层的输出与梯度都看作随机变量\n",
    "- 让它们的均值和方差都保持一致\n",
    "\n",
    "![20230809122858](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230809122858.png)\n",
    "\n",
    "a 和 b 都是常数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **权重初始化**\n",
    "- 在合理值区间里随机初始参数\n",
    "- 训练开始的时候更容易有数值不稳定\n",
    "  - 原理最优解的地方损失函数表面可能很复杂\n",
    "  - 最优解附近表面会比较平\n",
    "- 使用$\\mathscr{N}(0, 0.01)$来初始可能对小网络没问题，但不能保证深度神经网络\n",
    "\n",
    "[References](https://www.bilibili.com/video/BV1u64y1i75a/?p=2&share_source=copy_web&vd_source=a7ae9163cb2cd121bfd86ea1f4ecd2ef&t=331)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子：MLP\n",
    "- 假设\n",
    "  - $w^t_{i,j}$是i.i.d，那么$\\mathbb{E}[w^t_{i,j}] = 0$，$\\mathcal{Var}[w^t_{i,j}] = \\gamma_t$\n",
    "  - $h_i^{t-1}$独立于$w^t_{i,j}$\n",
    "- 假设没有激活函数$\\mathbf{h}^t = \\mathbf{W}^t\\mathbf{h}^{t-1}$，这里$\\mathbf{W}^t \\in \\mathbb{R}^{n_t \\times n_{t-1}}$\n",
    "$$\\mathbb{E}[\\mathbf{h}^t_i] = \\mathbb{E}[\\sum_{j} w^t_{i,j}h^{t-1}_j] = \\sum_{j} \\mathbb{E}[w^t_{i,j}]\\mathbb{E}[h^{t-1}_j] = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **正向方差**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathrm{Var}[h_i^t] & = E[(h_i^t)^2] - (E[h_i^t])^2 = \\mathbb{E}[(\\sum_{j} w_{i,j}^t h_j^{t-1})^2] \\\\\n",
    "        & = \\mathbb{E}[\\sum_{j} (w_{i,j}^t)^2 (h_j^{t-1})^2 + \\sum_{j \\neq k} w_{i,j}^t w_{i,k}^t h_j^{t-1} h_k^{t-1}] \\\\\n",
    "        & = \\sum_{j} \\mathbb{E} [(w_{i,j}^t)^2] \\mathbb{E}[(h_j^{t-1})^2] \\\\\n",
    "        & = \\sum_{j} \\mathrm{Var}[w_{i,j}^t] \\mathrm{Var}[h_j^{t-1}] \\\\\n",
    "        & = n_{t-1} \\gamma_t \\mathrm{Var}[h_j^{t-1}] \\\\\n",
    "        \\text{欲使} \\mathrm{Var}[h_i^t] = \\mathrm{Var}[h_j^{t-1}] \\qquad \\qquad & \\Rightarrow n_{t-1} \\gamma_t = 1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **反向均值和方差**\n",
    "- 跟正向情况类似\n",
    "$$\\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}^{t-1}} = \\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}^{t}} \\mathbf{W}^{t} \\quad \\Rightarrow \\quad (\\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}^{t-1}})^T = (\\mathbf{W}^{t})^T (\\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}^{t}})^T$$\n",
    "\n",
    "$$\\mathbb{E}[\\frac{\\partial \\mathscr{l}}{\\partial h^{t-1}_i}] = n_t \\gamma_t \\mathrm{Var}[\\frac{\\partial \\mathscr{l}}{\\partial h^{t}_i}] \\quad \\Rightarrow \\quad n_t \\gamma_t = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Xavier 初始**\n",
    "- **难以同时满足$n_{t-1} \\gamma_t = 1$和$n_t \\gamma_t = 1$，因为$n_{t-1} \\neq n_t$，二者表示的是前一层和当前层的神经元个数**\n",
    "- Xavier 进行了一个折中，使得$\\gamma_t(n_{t-1} + n_t) / 2 = 1 \\quad \\rightarrow \\quad \\gamma_t = 2 / (n_{t-1} + n_t)$\n",
    "  - 正态分布：$\\mathscr{N}(0, \\sqrt{2 / (n_{t-1} + n_t)})$\n",
    "  - 均匀分布：$\\mathscr{U}(-\\sqrt{6 / (n_{t-1} + n_t)}, \\sqrt{6 / (n_{t-1} + n_t)})$\n",
    "    - 分布$\\mathscr{U}[-a, a]$ 的方差为$a^2 / 3$\n",
    "- 适配权重形状变换，特别是$n_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **假设线性的激活函数**\n",
    "- 假设$\\sigma(x) = \\alpha x + \\beta$\n",
    "$$\\mathbf{h}' = \\mathbf{W} \\mathbf{h}^{t-1} \\quad \\text{and} \\quad \\mathbf{h}^{t} = \\sigma(\\mathbf{h}')$$\n",
    "\n",
    "$$\\mathbb{E}[h^t_i] = \\mathbb{E}[\\alpha h_i' + \\beta] = \\beta \\quad \\Rightarrow \\quad \\beta=0 $$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230809140633.png\" alt=\"20230809140633\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向\n",
    "- 假设 $\\sigma(x) = \\alpha x + \\beta$\n",
    "$$\\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}'} = \\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}^t} (W^t)^T \\quad \\text{and} \\quad \\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}^{t-1}} = \\alpha \\frac{\\partial \\mathscr{l}}{\\partial \\mathbf{h}'}$$\n",
    "\n",
    "$$\\mathbb{E}[\\frac{\\partial \\mathscr{l}}{\\partial h^{t-1}_i}] = 0 \\quad \\Rightarrow \\quad \\beta = 0$$\n",
    "\n",
    "$$\\mathrm{Var}[\\frac{\\partial \\mathscr{l}}{\\partial h^{t-1}_i}] = \\alpha^2 \\mathrm{Var}[\\frac{\\partial \\mathscr{l}}{\\partial h_j'}] \\quad \\Rightarrow \\quad \\alpha = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **总之，线性激活函数需要激活之后还是它本身**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查常用激活函数\n",
    "- 使用泰勒展开\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{sigmoid}(x) &= \\frac{1}{2} + \\frac{x}{4} - \\frac{x^3}{48} + O(x^5) \\\\\n",
    "\\tanh(x) &= 0 + x - \\frac{x^3}{3} + O(x^5) \\\\\n",
    "\\text{relu}(x) &= 0 + x \\quad \\text{for} \\quad x \\geq 0 \n",
    "\\end{align*}\n",
    "\n",
    "- 调整sigmoid(根据上述依据)，**只考虑 x = 0 邻域**：\n",
    "$$4 * sigmoid(x) - 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **总结**\n",
    "- **合理的权重初始值和激活函数的选取可以提升数值稳定性**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

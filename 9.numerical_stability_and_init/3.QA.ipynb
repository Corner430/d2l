{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**让每一层的方差保持恒定，而期望值为零，是神经网络中一种常见的正则化策略，被称为批标准化（Batch Normalization）**。这个技术的目标是加速神经网络的训练过程，增强网络的稳定性，以及一定程度上允许使用更高的学习率。\n",
    "\n",
    "批标准化的主要思想是在每个小批量数据上对网络的中间输出进行标准化，以使每一层的输入分布稳定在均值为 0 和方差为 1 的范围内。这个过程有几个主要的优点：\n",
    "\n",
    "1. **加速收敛：** 标准化每一层的输入分布有助于避免梯度消失或梯度爆炸的问题，从而加速网络的收敛速度。\n",
    "\n",
    "2. **允许更高的学习率：** 输入分布的稳定性意味着不同层的参数可以更快地收敛。这使得我们可以在训练过程中使用更大的学习率，加速收敛。\n",
    "\n",
    "3. **提高稳定性：** 标准化可以减少神经网络在训练过程中的震荡和不稳定性，使得模型更容易训练。\n",
    "\n",
    "4. **适应不同分布：** 批标准化可以让网络更容易适应不同尺度的输入分布，从而增强模型的泛化能力。\n",
    "\n",
    "需要注意的是，尽管批标准化在大多数情况下都有益处，**但并不是所有情况下都适用**。在一些情况下，批标准化可能会导致网络过早收敛，甚至导致性能下降。因此，在实际应用中，批标准化可能需要进行调整和实验，以确定是否适用于特定的网络结构和数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

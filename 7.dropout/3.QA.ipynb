{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以将模型设置的复杂一些，之后使用dropout，效果应该优于用一个不那么复杂的模型，不用dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **计算机体系结构中，n个数字相加，加的顺序不一样结果会不一样。所以用cuda算出来的结果每次都一定一样。（精度问题）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dropout 等价 regularization，**甚至可以将dropout和regularization同时使用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机种子\n",
    "在机器学习和统计学中，**随机种子（Random Seed）是一个起始值，它用于初始化随机数生成器，从而确定随机过程的起始状态**。随机种子可以影响随机事件的产生，使得在相同的随机种子下多次运行代码可以得到相同的随机结果，这在调试、验证和可复现性等方面非常有用。\n",
    "\n",
    "随机种子在机器学习中有着重要的作用，因为许多算法和模型涉及到随机性，例如初始化权重、数据采样和随机搜索等。**通过设置随机种子，您可以控制这些随机过程，使得每次运行实验的结果都是可复现的**。\n",
    "\n",
    "在深度学习中的丢弃法（Dropout）中，随机种子通常用来控制在网络训练过程中的随机丢弃行为。当您设置了相同的随机种子，每次训练时丢弃的神经元都是一样的，从而保证了模型的可重复性。这对于模型调整、超参数搜索和结果验证非常重要。\n",
    "\n",
    "在PyTorch等框架中，您可以通过设置随机种子来控制随机性。例如，在PyTorch中，您可以使用以下方式来设置随机种子：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "```\n",
    "\n",
    "这将确保使用了相同的随机种子，每次运行代码时都会得到相同的随机结果。请注意，不同的库和模块可能有不同的设置方式，具体要参考您正在使用的文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么 dropout 可能会导致参数收敛变慢？\n",
    "dropout 可能会导致参数收敛变慢，尤其是在模型较深、数据较少或 dropout 比例较高的情况下。\n",
    "\n",
    "Dropout 是一种正则化技术，它通过随机地在训练过程中丢弃一些神经元的输出，以减少网络的过拟合。然而，这种随机丢弃也会导致在每次训练迭代中，模型只能观察到部分神经元的输出，从而可能导致训练过程更加不稳定，使参数调整和收敛的速度变慢。\n",
    "\n",
    "特别是在初始阶段，随机的 dropout 可能会导致一些神经元的输出被强制丢弃，使得模型无法正常地学习。这可能会导致训练损失变化较大，参数更新不稳定，从而影响收敛的速度。\n",
    "\n",
    "**为了缓解这种问题，通常会采用一些策略来使用 dropout：**\n",
    "- **逐渐增加 dropout 比例**：可以从较小的 dropout 比例开始，然后逐渐增加，以便在训练过程稳定后引入更多的随机性。\n",
    "- **使用合适的学习率**：由于 dropout 可能会使训练过程变得不稳定，选择合适的学习率尤为重要，以防止参数更新过大或过小。\n",
    "\n",
    "总的来说，dropout 在一些情况下可能会影响参数的收敛速度，但在控制过拟合、提高模型泛化能力方面仍然是一种有用的正则化方法。在实际应用中，通常需要进行实验和调整，以找到适合具体任务和模型的 dropout 比例和训练策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不使用正则化\n",
    "\n",
    "![20230808223645](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230808223645.png)\n",
    "\n",
    "2. 使用权重衰退\n",
    "\n",
    "![20230808223714](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230808223714.png)\n",
    "\n",
    "3. 使用dropout\n",
    "\n",
    "![20230808223739](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230808223739.png)\n",
    "\n",
    "4. 混合使用\n",
    "\n",
    "![20230808223757](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230808223757.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

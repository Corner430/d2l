{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 丢弃法(dropout)\n",
    "### 动机\n",
    "- **一个好的模型需要对输入数据的扰动鲁棒**\n",
    "  - 使用有噪音的数据等价于Tikhonov正则（类似于岭回归）\n",
    "  - 丢弃法：**在层之间加入噪音**，它的**原理**是在模型的训练过程中，**随机地将一些神经元的输出置零**，从而降低网络对某些特定神经元的过度依赖。这种技术在某种程度上类似于在层之间引入噪音。通过在训练过程中随机地“丢弃”一些神经元，**模型被迫在不同的子集上进行学习，从而提高了模型对于输入数据扰动的鲁棒性**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无偏差的加入噪音\n",
    "- 对$\\mathbf{x}$加入噪音得到$\\mathbf{x'}$，**我们希望**\n",
    "\n",
    "$$E[\\mathbf{x'}] = \\mathbf{x}$$\n",
    "\n",
    "- 丢弃法对每个元素进行如下扰动\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_i' =\n",
    "\\begin{cases}\n",
    "    0 & \\text{ 概率为 } p \\\\\n",
    "    \\frac{x_i}{1-p} & \\text{ 其他情况}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- 这样的设计，**有$E[x_i'] = p * 0 + (1 - p) * \\frac{x_i}{1-p} = x_i$，因此丢弃法不改变输入的期望值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用丢弃法\n",
    "- **在训练过程中**，通常将丢弃法作用在隐藏全连接层的输出上\n",
    "\n",
    "![dropout前后的多层感知机](img/dropout2.svg)\n",
    "\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1)$$\n",
    "$$\\mathbf{h'} = \\text{dropout}(\\mathbf{h})$$\n",
    "$$\\mathbf{o} = \\sigma(\\mathbf{W}_2\\mathbf{h'} + \\mathbf{b}_2)$$\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{o})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理（预测）中的丢弃法\n",
    "- 正则项只在训练中使用：他们影响模型参数的更新\n",
    "- 在推理过程中，丢弃法直接返回输入\n",
    "$$\\mathbf{h} = \\text{dropout}(\\mathbf{h})$$\n",
    "- 这样也能保证确定性的输出\n",
    "\n",
    "> **也就是说，正则项只在训练中使用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 丢弃法将一些输出项随机置0来控制模型复杂度\n",
    "- 常作用在多层感知机的隐藏层输出上\n",
    "- 丢弃概率是控制模型复杂度的超参数，**一般取值为0.2~0.5**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

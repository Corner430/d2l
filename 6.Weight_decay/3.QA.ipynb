{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于上述代码，使用$L_1$范数正则化，效果会非常好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实际中的权重衰退的值一般取1e-2、1e-3、1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **权重衰退所能带来的效果很有限**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为什么我们首先使用$L_2$范数，而不是$L_1$范数呢？\n",
    "  - 事实上，这个选择在整个统计学领域都是有效的受欢迎的。\n",
    "  - $L_2$正则化线性模型构成经典的**岭回归(ridge regression)**算法，而$L_1$正则化线性模型构成了**套索回归(Lasso regression)**算法。\n",
    "  - 这使得$L_2$范数对权重向量的大分量施加了巨大的惩罚，相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上，而将其他权重清除为零，这称为**特征选择(feature selection)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_1$ 正则化之所以会将一些权重参数变为 0，涉及到优化问题中的几何性质\n",
    "\n",
    "- 在没有正则化的情况下，优化问题可以被视为在参数空间中寻找损失函数的最小值。每个参数代表了一个维度，而优化过程则是在多维空间中寻找一个最低点，即最小化损失函数。\n",
    "\n",
    "- $L_1$ 正则化的损失函数是由原始损失函数和一个绝对值惩罚项组成的。绝对值惩罚项的几何性质是，它的等值线是一个“菱形”（$L_1$ 范数的等值线）。现在，假设我们在参数空间中进行优化，并且只考虑两个参数的情况（二维空间）。\n",
    "\n",
    "- 当没有正则化时，我们要在参数空间中寻找一个最低点，这可能是一个凹形的“碗状”区域。而当引入了 $L_1$ 正则化时，绝对值惩罚项的“菱形”等值线与这个碗状区域相交在一些点上。但是，“菱形”的角部是在坐标轴上，而这些点是参数空间中的零点。\n",
    "\n",
    "- 由于 $L_1$ 正则化项的“菱形”等值线相对于碗状区域的边缘更可能与坐标轴相交，因此在优化过程中，算法更有可能在这些坐标轴上找到最小值，即将参数置为零。这就是为什么 $L_1$ 正则化倾向于使得一些权重参数变为零的原因。\n",
    "\n",
    "- 总结起来，$L_1$ 正则化引入的“菱形”等值线结构使得优化算法更容易在坐标轴上找到最小值，导致一些权重参数变为零，从而实现了特征选择。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

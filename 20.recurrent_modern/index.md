1. [GRU](1.gru.ipynb)
    - **门控循环单元与普通的循环神经网络之间的关键区别在于：前者支持隐状态的门控。**
    - **重置门有助于捕获序列中的短期依赖关系；**
    - **更新门有助于捕获序列中的长期依赖关系。**
2. [LSTM](2.lstm.ipynb)
    - 长短期记忆网络有三种类型的门：**输入门、遗忘门和输出门。**
    - 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
    - 长短期记忆网络**可以缓解梯度消失和梯度爆炸。**
3. [Deep RNN](3.deep-rnn.ipynb)
    - **在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。**
    - **有许多不同风格的深度循环神经网络，**如长短期记忆网络、门控循环单元、或经典循环神经网络。这些模型在深度学习框架的高级API中都有涵盖。
    - 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪）来确保合适的收敛，**模型的初始化也需要谨慎。**
4. [Bi RNN](4.bi-rnn.ipynb)
    - **双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。**
    - **隐状态 H 是将正向和反向隐状态串联起来的结果。**
    - **在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。**
    - 双向循环神经网络的计算速度非常慢
5. [Machine Translation and Dataset](5.machine-translation-and-dataset.ipynb)
    - 机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。
    - **使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。**
    - **通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。**
6. [Encoder Decoder](6.encoder-decoder.ipynb)
    - “编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
    - 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
    - 解码器将具有固定形状的编码状态映射为长度可变的序列。
7. [Seq2Seq](7.seq2seq.ipynb)
    - 根据“编码器-解码器”架构的设计，我们**可以使用两个循环神经网络来设计一个序列到序列学习的模型。**
    - **在实现编码器和解码器时，我们可以使用多层循环神经网络。**
    - 我们**可以使用遮蔽来过滤不相关的计算**，例如在计算损失时。
    - 在“编码器－解码器”训练中，**强制教学方法将原始输出序列（而非预测结果）输入解码器。**
    - **BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的$n$元语法的匹配度来评估预测。**
8. [Beam Search](8.beam-search.ipynb)
    - 序列搜索策略包括贪心搜索、穷举搜索和束搜索。
    - 贪心搜索所选取序列的计算量最小，但精度相对较低。
    - 穷举搜索所选取序列的精度最高，但计算量最大。
    - 束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。
1. [GRU](1.gru.ipynb)
    - **门控循环单元与普通的循环神经网络之间的关键区别在于：前者支持隐状态的门控。**
    - **重置门有助于捕获序列中的短期依赖关系；**
    - **更新门有助于捕获序列中的长期依赖关系。**
2. [LSTM](2.lstm.ipynb)
    - 长短期记忆网络有三种类型的门：**输入门、遗忘门和输出门。**
    - 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
    - 长短期记忆网络**可以缓解梯度消失和梯度爆炸。**
3. [Deep RNN](3.deep-rnn.ipynb)
    - **在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。**
    - **有许多不同风格的深度循环神经网络，**如长短期记忆网络、门控循环单元、或经典循环神经网络。这些模型在深度学习框架的高级API中都有涵盖。
    - 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪）来确保合适的收敛，**模型的初始化也需要谨慎。**
4. [Bi RNN](4.bi-rnn.ipynb)
    - **双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。**
    - **隐状态 H 是将正向和反向隐状态串联起来的结果。**
    - **在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。**
    - 双向循环神经网络的计算速度非常慢
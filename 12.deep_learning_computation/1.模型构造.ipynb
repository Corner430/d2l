{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络研究人员已经从考虑单个人工神经元的行为转变为从**层**的角度构思网络，通常在设计架构时考虑的是更粗粒度的**块（block）**\n",
    "\n",
    "- 事实证明，研究讨论“比单个层大”但\"比整个模型小\"的组件更有价值。例如，在计算机视觉中广泛流行的 ResNet-152 架构就有数百层，**这些层是由层组（group of layers）的重复模式组成**\n",
    "\n",
    "- **神经网络块的概念：块（block）可以描述单个层、由多个层组成的组件，或整个模型本身**\n",
    "\n",
    "- 从编程的角度来看，**块由类（class）表示，我们只需要考虑前向传播函数和必要的参数即可。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`nn.Sequential`定义了一种特殊的`Module`，即在Pytorch中表示一个块的类，它维护一个由`Module`组成的有序列表**\n",
    "\n",
    "- **`net(X)`实际上是`net.__call__(X)`的简写**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **块（block）必须提供的基本功能**：\n",
    "  1. **将输入数据作为其前向传播函数的参数**\n",
    "  2. 通过前向传播函数来生成输出\n",
    "  3. **计算其输出关于输入的梯度，可通过反向传播函数进行访问。通常这是自动完成的**\n",
    "  4. **存储和访问**前向传播计算所需的参数\n",
    "  5. **根据需要初始化模型参数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "- **一个块可以由许多层组成；一个块可以由许多块组成**\n",
    "- 块可以包含代码\n",
    "- 块负责大量的内部处理，**包括参数初始化和反向传播**\n",
    "- 层和块的顺序连接由`Sequential`块处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们回顾一下**多层感知机**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0242,  0.1153,  0.2040,  0.0287, -0.0266,  0.0159,  0.0609,  0.0128,\n",
       "          0.2768,  0.1767],\n",
       "        [-0.0182,  0.0165,  0.1345,  0.0110, -0.0769, -0.0263,  0.0359,  0.0306,\n",
       "          0.1647,  0.1684]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    \n",
    "    # 定义模型的前向传播，即如何根据输入X计算返回所需要的模型输出\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0072, -0.2892,  0.1307, -0.0346,  0.0774,  0.3824,  0.0403,  0.0860,\n",
       "          0.1285,  0.0466],\n",
       "        [-0.0055, -0.2632,  0.1058, -0.0091,  0.0549,  0.1241, -0.0058, -0.1128,\n",
       "          0.1322,  0.0235]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 顺序块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了构建我们自己的简化的`MySequiential`，只需要定义下面**两个关键函数**：\n",
    "\n",
    "  1. **将块逐个追加到列表中的函数**\n",
    "  2. **前向传播函数**，用于将输入按追加块的顺序传递给块组成的“链条”\n",
    "\n",
    "> **`MySequential`的用法与`Sequential`一样**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1484, -0.0229,  0.0901, -0.0608,  0.0258,  0.0669, -0.1206, -0.0817,\n",
       "         -0.1542,  0.1571],\n",
       "        [ 0.0770,  0.0306,  0.0601, -0.0318,  0.1206,  0.0801, -0.1502, -0.0202,\n",
       "         -0.0664,  0.1646]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    # 提供了与Sequential类相同的功能\n",
    "    # def __init__(self, *args):\n",
    "        # super().__init__()\n",
    "        # for idx, module in enumerate(args):\n",
    "            # # 这里，`module`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量\n",
    "            # # 变量`_modules` 中。`_modules` 的类型是 OrderedDict。\n",
    "            # self._modules[str(idx)] = module\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block    # 这里_modules是OrderedDict类型\n",
    "            # `_modules`的主要优点是：在模块的参数初始化过程中，系统知道在`_modules`字典中查找相应的模块。\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在正向传播函数中执行代码，有时`torch.Sequential`无法满足需求，需要我们自定义函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2683, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)    # 不可训练参数\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = F.relu(torch.mm(x, self.rand_weight) + 1)\n",
    "        x = self.linear(x)\n",
    "        while x.abs().sum() > 1:\n",
    "            x /= 2\n",
    "        return x.sum()  # 返回标量\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**组合搭配各种组合块方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3256, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

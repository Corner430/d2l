{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数管理\n",
    "我们首先关注具有单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0811],\n",
       "        [0.1080]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参数访问**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1908, -0.3356,  0.3059, -0.0915, -0.2010, -0.0697,  0.2919,  0.2058]])), ('bias', tensor([0.0331]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())  # state_dict()返回一个从参数名称映射到参数Tensor的字典，也是一个OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目标参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0331], requires_grad=True)\n",
      "tensor([0.0331])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))    # Parameter是一个可以优化的参数类型\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None  # 还没有计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一次性访问所有参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "# 1. `net[0]`表示模型中的第一个子模块。神经网络模型通常由许多层（或子模块）组成，通过索引来访问特定的子模块。这里的`net[0]`表示访问模型中的第一个子模块。\n",
    "# 2. `net[0].named_parameters()`返回一个生成器，该生成器会产生子模块（在这种情况下是第一个子模块）的参数及其名称。每个参数都是一个张量，而名称是一个字符串。\n",
    "# 3. `name`是参数的名称，`param`是参数的张量。\n",
    "# 4. `[(name, param.shape) for name, param in net[0].named_parameters()]`是一个列表推导式，它遍历`net[0]`子模块的参数，并为每个参数生成一个元组，元组包含参数的名称和形状。\n",
    "# 5. `print(*[(name, param.shape) for name, param in net[0].named_parameters()])`使用`print`函数打印了这个列表中的每个元组。`*`符号用于展开列表，使其成为`print`函数的多个参数，从而以易读的方式打印出参数名称和形状。\n",
    "# 6. `net.named_parameters()`返回一个生成器，该生成器会产生整个模型的所有参数及其名称。每个参数都是一个张量，而名称是一个字符串。\n",
    "# 7. 同样地，`[(name, param.shape) for name, param in net.named_parameters()]`是一个列表推导式，它遍历整个模型的参数，并为每个参数生成一个元组，元组包含参数的名称和形状。\n",
    "# 8. `print(*[(name, param.shape) for name, param in net.named_parameters()])`使用`print`函数打印了整个模型的所有参数的名称和形状。\n",
    "# 综上所述，这段代码用于打印模型中特定子模块（例如第一个子模块）以及整个模型的参数名称和形状，以帮助你了解模型的结构和参数情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.4256, -0.3368,  0.0355, -0.2054],\n",
       "                      [-0.4452, -0.3629,  0.3095,  0.0042],\n",
       "                      [-0.0308,  0.0642, -0.0702, -0.3943],\n",
       "                      [-0.3609, -0.0623,  0.2616, -0.1050],\n",
       "                      [ 0.0984,  0.4006,  0.1920, -0.0916],\n",
       "                      [ 0.0178, -0.2304,  0.3083,  0.0837],\n",
       "                      [ 0.0703,  0.4019, -0.2974, -0.0267],\n",
       "                      [ 0.0371,  0.0272, -0.1045, -0.3323]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.2332, -0.4727,  0.0624,  0.4416, -0.2444,  0.3194,  0.4213,  0.0194])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.1908, -0.3356,  0.3059, -0.0915, -0.2010, -0.0697,  0.2919,  0.2058]])),\n",
       "             ('2.bias', tensor([0.0331]))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0331])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从嵌套块收集参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4297],\n",
       "        [-0.4297]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block{i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**余弦学习率（Cosine Learning Rate）** 是一种动态学习率调度方法，用于在训练过程中逐渐降低学习率，从而帮助模型更好地收敛到最优解。余弦学习率的特点是在训练的不同阶段以余弦函数的形式调整学习率，使学习率从一个较大的值逐渐减小到一个较小的值，然后保持较小的值。\n",
    "\n",
    "具体来说，余弦学习率按照如下公式计算：\n",
    "\n",
    "$$\\text{lr}(t) = \\frac{\\text{lr}_{\\text{max}}}{2} \\left(1 + \\cos\\left(\\frac{t}{T}\\pi\\right)\\right)$$\n",
    "\n",
    "其中：\n",
    "- $t$ 是当前训练的迭代次数或轮数。\n",
    "- $\\text{lr}_{\\text{max}}$ 是初始的最大学习率，一般为一个较大的正数。\n",
    "- $T$ 是一个超参数，表示一个学习率更新周期的长度。\n",
    "\n",
    "余弦学习率调度方法的核心思想是从初始的最大学习率开始，然后按照余弦函数的形式逐渐降低学习率，使模型在训练过程中先进行较大的步长搜索，然后逐渐减小步长进行更精细的搜索，有助于提高模型在训练后期的稳定性和性能。\n",
    "\n",
    "除了余弦学习率，还有许多其他的学习率调度方法，常见的有：\n",
    "\n",
    "1. **学习率衰减（Learning Rate Decay）**：在每个固定的训练步骤或周期后，将学习率乘以一个小于1的因子，以逐渐减小学习率。\n",
    "\n",
    "2. **指数衰减学习率（Exponential Decay Learning Rate）**：将学习率按照指数函数进行衰减，可以根据迭代次数或轮数进行调整。\n",
    "\n",
    "3. **Step 学习率衰减（Step Decay Learning Rate）**：在特定的训练步骤或周期后，将学习率减小为原来的一小部分，用于调整学习率。\n",
    "\n",
    "4. **自适应方法（Adaptive Methods）**：例如 Adam、RMSProp 等，根据每个参数的梯度自适应地调整学习率，不需要手动设置学习率。\n",
    "\n",
    "不同的学习率调度方法适用于不同的训练场景，选择合适的学习率调度方法可以帮助模型更快地收敛并获得更好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，**残差（Residual）** 是指在神经网络中，某一层的输出与输入之间的差异。残差网络（Residual Network，通常简称为 ResNet）则是一种特殊的神经网络结构，通过引入残差块来改进网络的训练和性能。\n",
    "\n",
    "深层神经网络在训练过程中可能会面临梯度消失和梯度爆炸等问题，尤其是在网络层数较多的情况下。这些问题会导致训练难度增加，模型收敛缓慢，或者在一些层之间的信息传递受到阻碍。为了解决这些问题，残差网络引入了**跳跃连接**（Skip Connection）和**残差块**（Residual Block）的概念。\n",
    "\n",
    "在一个残差块中，输入 \\(x\\) 通过一系列神经网络层进行变换，然后与输入本身相加，得到输出 \\(F(x) + x\\)。这种跳跃连接使得模型可以学习残差，即网络应该学习如何将输入转化为零均值的残差。这种结构使得网络在训练过程中更容易优化，因为即使某些层学习到了恒等映射，仍然可以通过跳跃连接将残差传递给下一层。这也有助于更快地训练深层网络。\n",
    "\n",
    "在数学上，一个残差块可以表示为：\n",
    "\n",
    "$$\\text{output} = F(\\text{input}) + \\text{input}$$\n",
    "\n",
    "其中，$\\text{input}$ 是输入，$F(\\text{input})$ 是经过一系列神经网络层变换后的输出。\n",
    "\n",
    "> **简而言之，模型先去拟合 x, 也就是先去拟合底层的小模型，之后再去学习残差**，原因在于梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为什么定义两个 `bn`，因为两个各自有自己的参数"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

1. [AlexNet](1.AlexNet.ipynb)
    - AlexNet的出现
2. [代码](2.代码.ipynb)
3. [QA](3.QA.ipynb)
4. [VGG](4.VGG.ipynb)
    - 更大更深的AlexNet（重复的VGG块）
    - 进度
5. [代码](5.代码.ipynb)
    - 深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效
    - 与AlexNet相比，**VGG的计算要慢得多**，而且它还需要更多的显存
6. [QA](6.QA.ipynb)
7. [NiN](7.NiN.ipynb)
    - **卷积层后的第一个全连接层带来的问题**
    - **NiN 在每个像素的通道上分别使用多层感知机**
    -  **NiN架构**
        - 无全连接层
        - 最后使用**全局平均池化层**得到输出，**其输入通道数是类别数**，不容易过拟合，更少的参数个数
8. [代码](8.代码.ipynb)
9. [QA](9.QA.ipynb)
    - 收敛变慢
10. [GoogLeNet](10.GoogLeNet.ipynb)
    - GoogLeNet 将各种卷积超参数都用上了，是一个**含并行连结的网络**
    - 它的一个**主要优点是模型参数小，计算复杂度低**
    - Inception 块相当于一个有 4 条路径的子网络
11. [代码](11.代码.ipynb)
12. [QA](12.QA.ipynb)
    - 为什么 GoogLeNet 这个网络如此有效呢？
    - Inception 块的通道数分配之比是在 ImageNet 数据集上通过大量的实验得来的
13. [批量归一化](13.批量归一化.ipynb)
    - 批量归一化层解决的问题
    - 批量归一化层作用范围
    - 批量归一化层在卷积层为什么作用在通道维度
    - **批量归一化在做什么**
    - **可以加速收敛速度，但一般不改变模型精度**
14. [代码](14.代码.ipynb)
    - 方差的计算
    - 指数加权平均
15. [QA](15.QA.ipynb)
    - **标准化（Normalization）、归一化（Scaling）和 批量归一化（Batch Normalization）的区别**
    - **批量归一化中 $\gamma$ 和 $\beta$ 的作用**
    - 为什么使用批量规范化？
16. [ResNet](16.ResNet.ipynb)
    - **ResNet 解决了模型偏差的问题，使用了模型（函数）嵌套。也就是扩大了函数类。**
17. [代码](17.代码.ipynb)
18. [QA](18.QA.ipynb)
    - **学习率调度**
    - 所谓残差
19. [ResNet为什么能训练出1000层的模型？](19.ResNet为什么能训练出1000层的模型.ipynb)
20. [Densenet](20.DenseNet.ipynb)
    - 稠密块的**核心思想是：每个卷积层的输入都包含了前面所有层的输出**。这意味着，每个卷积层都能够访问到网络中所有层的信息，从而增强特征的重用和梯度流动。
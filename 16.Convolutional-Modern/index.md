1. [AlexNet](1.AlexNet.ipynb)
    - AlexNet的出现
2. [代码](2.代码.ipynb)
3. [QA](3.QA.ipynb)
4. [VGG](4.VGG.ipynb)
    - 更大更深的AlexNet（重复的VGG块）
    - 进度
5. [代码](5.代码.ipynb)
    - 深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效
    - 与AlexNet相比，**VGG的计算要慢得多**，而且它还需要更多的显存
6. [QA](6.QA.ipynb)
7. [NiN](7.NiN.ipynb)
    - **卷积层后的第一个全连接层带来的问题**
    -  **NiN架构**
        - 无全连接层
        - 最后使用**全局平均池化层**得到输出，**其输入通道数是类别数**，不容易过拟合，更少的参数个数
8. [代码](8.代码.ipynb)
9. [QA](9.QA.ipynb)
    - 收敛变慢
10. [GoogLeNet](10.GoogLeNet.ipynb)
    - GoogLeNet 将各种卷积超参数都用上了，是一个**含并行连结的网络**
    - 它的一个**主要优点是模型参数小，计算复杂度低**
11. [代码](11.代码.ipynb)
12. [QA](12.QA.ipynb)
13. [批量归一化](13.批量归一化.ipynb)
    - 批量归一化层解决的问题
    - 批量归一化层作用范围
    - 批量归一化层在卷积层为什么作用在通道维度
    - **批量归一化在做什么**
    - **可以加速收敛速度，但一般不改变模型精度**
14. [代码](14.代码.ipynb)
    - 方差的计算
    - 指数加权平均
15. [QA](15.QA.ipynb)
    - **标准化（Normalization）、归一化（Scaling）和 批量归一化（Batch Normalization）的区别**
    - **批量归一化中 $\gamma$ 和 $\beta$ 的作用**
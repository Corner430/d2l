{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化（Normalization）和归一化（Scaling）是数据处理中常用的两种技术，用于将数据调整到特定的范围或分布。它们在不同的上下文中有不同的含义。\n",
    "\n",
    "**标准化（Normalization）**：\n",
    "标准化通常是指将数据转换为均值为0，标准差为1的分布，也称为Z-score标准化。这个过程能够使得数据的分布更加接近正态分布，有助于某些统计分析和机器学习模型的表现。标准化的数学表达式为：\n",
    "\n",
    "$$X_{\\text{normalized}} = \\frac{X - \\text{mean}(X)}{\\text{std}(X)}$$\n",
    "\n",
    "其中，$X$ 表示原始数据，$\\text{mean}(X)$ 表示数据的均值，$\\text{std}(X)$ 表示数据的标准差。\n",
    "\n",
    "**归一化（Scaling）**：\n",
    "归一化通常是指将数据线性映射到一个特定的范围，例如[0, 1]或[-1, 1]。这个过程有助于确保不同特征的值在相同的尺度范围内，防止某些特征的值过大对模型造成不利影响。归一化的数学表达式为：\n",
    "\n",
    "$$X_{\\text{normalized}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}$$\n",
    "\n",
    "其中，$X$ 表示原始数据，$\\text{min}(X)$ 和 $\\text{max}(X)$ 分别表示数据的最小值和最大值。\n",
    "\n",
    "**批归一化（Batch Normalization）**：\n",
    "批归一化是一种用于加速深度神经网络训练和提高模型稳定性的技术。它在每一层的输入上进行归一化操作，使得中间层的输出的均值和方差保持稳定。具体而言，批归一化会计算当前批次内的均值和方差，然后对输入进行标准化，再进行缩放和平移操作。这有助于解决梯度消失和梯度爆炸问题，加速模型训练，以及增强模型的泛化能力。\n",
    "\n",
    "批归一化的公式如下：\n",
    "\n",
    "$$\\mathbf{x}_{i+1} = \\gamma \\frac{\\mathbf{x}_i - \\boldsymbol{\\mu}_\\mathcal{B}}{\\boldsymbol{\\sigma}_\\mathcal{B}} + \\beta$$\n",
    "\n",
    "其中，$\\mathbf{x}_i$ 是输入数据，$\\gamma$ 和 $\\beta$ 是可训练的缩放和偏移参数，$\\boldsymbol{\\mu}_\\mathcal{B}$ 和 $\\boldsymbol{\\sigma}_\\mathcal{B}$ 是当前批次内的均值和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "$\\gamma$ 和 $\\beta$ 是批归一化（Batch Normalization）中的可训练参数，用于调整标准化后的数据，从而增加网络的表达能力和灵活性。它们分别对应于标准化后的数据的缩放和平移操作，具体来说：\n",
    "\n",
    "1. **$\\gamma$（缩放参数）**：这个参数用于缩放标准化后的数据。通过调整 $\\gamma$，可以控制每个通道或特征的重要性和尺度。如果一个特征对模型的重要性较大，可以通过适当增大 $\\gamma$ 来放大这个特征的影响；反之，如果某个特征相对不重要，可以通过适当减小 $\\gamma$ 来减弱这个特征的影响。\n",
    "\n",
    "2. **$\\beta$（平移参数）**：这个参数用于平移标准化后的数据。通过调整 $\\beta$，可以在不改变数据的分布形状的情况下，对数据进行适当的平移，以适应不同的情况。平移参数 $\\beta$ 的主要作用是为了保留网络层中原始数据的一些信息，使网络能够学习在某些情况下不进行标准化的权重。\n",
    "\n",
    "这两个参数的引入使得批归一化层变得更加灵活。如果 $\\gamma$ 和 $\\beta$ 的初始值分别为 1 和 0，那么在网络初始阶段，批归一化不会对数据做额外的缩放和平移，因为它们的作用是可学习的，会在训练过程中进行调整。\n",
    "\n",
    "总之，$\\gamma$ 和 $\\beta$ 这两个可训练参数使得批归一化层能够学习适合数据分布的缩放和平移，从而增强了网络的表达能力，帮助网络更好地适应不同任务的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "**Batch Normalization 在网络越深的时候，效果越明显**。因为在深层网络中，数据的分布会发生变化，而 Batch Normalization 能够使得数据的分布保持稳定，从而加速网络的训练，提高模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "Batch Normalization 使得分布稳定，**那么就可以使用一个统一的学习率进行训练**，而不需要对每一层都使用不同的学习率。这样可以加快模型的训练速度，同时也能够提高模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "- 为什么需要批量规范化层呢？\n",
    "  - 数据预处理的方式通常会对最终结果产生巨大影响\n",
    "  - 对于典型的多层感知机或卷积神经网络，当训练时，中间层的变量可能具有更广的变化范围。**批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛**。直观地说，我们可能会猜测，如果一个层的可变值是另一层的 100 倍，这可能**需要对学习率进行补偿调整**。\n",
    "  - 更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "如果我们尝试使用大小为 1 的小批量应用批量规范化，将无法学习到任何东西。在应用批量规范化时，批量大小的选择可能比我们在没有批量规范化时使用的大小更重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "- 批量规范化层在“训练模式”（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。\n",
    "  - 在训练模式下，我们**无法使用**整个数据集来估计均值和方差，所以**只能**根据每个小批量的均值和方差不断训练模型\n",
    "  - 而在预测模式下，**可以**根据整个数据集精确计算批量规范化所需的均值和方差"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

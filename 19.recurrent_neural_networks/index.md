1. [Sequence](1.sequence.ipynb)
    - 时序模型中，当前数据跟之前观察到的数据相关
    - 自回归模型使用自身过去数据来预测未来
    - 马尔可夫模型假设当前只跟最近少数数据相关，从而简化模型
    - 潜变量模型使用潜变量来概括历史信息
    - 内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。
    - 序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。
    - 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。
    - **对于直到时间步$t$的观测序列，其在时间步$t+k$的预测输出是“$k$步预测”。随着我们对预测时间$k$值的增加，会造成误差的快速累积和预测质量的极速下降。**
    - $tau$ 并不是越大越好，极端来说，当 $tau$ 等于序列长度时，就只有一个样本，这样就没有意义了。
2. [Text Preprocessing](2.text-preprocessing.ipynb)
    - **H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)**
3. [Lauguage Model and Dataset](3.language-models-and-dataset.ipynb)
    - 马尔可夫模型与 n 元语法
    - 单词的频率满足**齐普夫定律（Zipf's law）**
    - **读取长序列的主要方式是随机采样和顺序分区**
4. [RNN](4.rnn.ipynb)
    - **隐状态中$\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}$的计算，相当于$\mathbf{X}_t$和$\mathbf{H}_{t-1}$的拼接与$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接的矩阵乘法。**
    - **隐藏层和隐状态指的是两个截然不同的概念。**
    - **循环神经网络模型的参数数量不会随着时间步的增加而增加。**
    - **我们可以使用困惑度来评价语言模型的质量。**
    - [更多的应用 RNNs](https://www.bilibili.com/video/BV1D64y1z7CA/?share_source=copy_web&vd_source=a7ae9163cb2cd121bfd86ea1f4ecd2ef&t=1142)
5. [Rnn Scratch](5.rnn-scratch.ipynb)
    - **循环神经网络模型在训练以前需要初始化状态，不过随机抽样和顺序划分使用初始化方法不同。**
    - **当使用顺序划分时，我们需要分离梯度以减少计算量。**
    - **在进行任何预测之前，模型通过预热期进行自我更新（例如，获得比初始值更好的隐状态）。**
    - **[梯度裁剪](https://www.bilibili.com/video/BV1D64y1z7CA/?share_source=copy_web&vd_source=a7ae9163cb2cd121bfd86ea1f4ecd2ef&t=929)可以防止梯度爆炸，但不能应对梯度消失。**
    - 为什么是 批量大小乘时间长度？
    - 为什么是预测字符而不是预测一个单词呢？
6. [Rnn Concise](6.rnn-concise.ipynb)
    - **高级API的循环神经网络层返回一个输出和一个更新后的隐状态，我们还需要计算整个模型的输出层。**
    - **相比从零开始实现的循环神经网络，使用高级API实现可以加速训练。**
7. [bptt](7.bptt.ipynb)
    - “通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。
    - **截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。**
    - 矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。
    - **为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。**